#!/usr/bin/env python 

import json
import sys
import pprint

from bs4 import BeautifulSoup
import requests

ROOT = 'http://ohhla.com/'

URLS = [ROOT + page for page in [
  'all.html',
  'all_two.html',
  'all_three.html',
  'all_four.html',
  'all_five.html',
]]



def index_links (pre):
  def extract_link(a):
    try:
      return a['href'], a.string
    except:
      return None

  links = [extract_link(a) for a in pre.find_all('a')]
  return filter(None, links)


def gen_links(urls):
  for url in urls:
    print >>sys.stderr, url
    response = requests.get(url)
    html = response.text
    soup = BeautifulSoup(html)

    raw_index = {
        'html': html,
        'pre': soup.find('pre'),
    }

    yield index_links(soup.find('pre'))


def flatten1(lists):
  return sum(lists, [])

if __name__ == '__main__':
  links = list(gen_links(URLS))
  links = flatten1(links)
  print json.dumps(links)
